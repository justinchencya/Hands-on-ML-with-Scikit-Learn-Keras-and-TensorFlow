{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "# Kernel will die if we don't run this line\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# # Where to save the figures\n",
    "# PROJECT_ROOT_DIR = \".\"\n",
    "# CHAPTER_ID = \"deep\"\n",
    "# IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "# os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "# def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "#     path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "#     print(\"Saving figure\", fig_id)\n",
    "#     if tight_layout:\n",
    "#         plt.tight_layout()\n",
    "#     plt.savefig(path, format=fig_extension, dpi=resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients Problems \n",
    "- The **Vanishing Gradients** Problem\n",
    "    - When training deep neural networks, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. \n",
    "    - As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution.\n",
    "- The **Exploding Gradients** Problem\n",
    "    - In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glorot and He Initialization\n",
    "- To significantly alleviate the Vanishing/Exploding Gradient problems, Glorot and Bengio proposed that we need the signal to flow properly in both directions.\n",
    "- For the signal to flow properly, we need the **variance of the outputs** of each layer to be **equal** to the **variance of its inputs**, and we also need the **gradients to have equal variance** before and after flowing through a layer in the reverse direction. \n",
    "- It is actually not possible to guarantee both unless the layer has **an equal number of inputs and neurons** (these numbers are called the **fan-in** and **fan-out** of the layer), but they proposed a good compromise that has proven to work very well in practice - **Glorot (Xavier) Initialization**.\n",
    "    - The connection weights of each layer must be initialized randomly as:\n",
    "        - **Normal distribution** with mean 0 and variance $\\sigma^2 = 1/fan_{avg}$\n",
    "        - Or a **uniform distribution** between $-r$ and $+r$, with $r = \\sqrt{\\frac{3}{fan_{avg}}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'serialize',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He Initialization\n",
    "- Activation functions: ReLU & variants\n",
    "- $\\sigma^2$ (normal): $\\frac{2}{fan_{in}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fce93e1a5d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you want **He initialization** with a **uniform distribution**, but based on $fan_{avg}$ rather than $fan_{in}$, you can use the `VarianceScaling` initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fce93eb9350>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary - Initialization parameters for each type of activation function\n",
    "- **Glorot**\n",
    "    - Activation functions: None, Tanh, Logistic, Softmax\n",
    "    - $\\sigma^2 (Normal) = 1/fan_{avg}$\n",
    "- **He**\n",
    "    - Activation functions: ReLU & variants\n",
    "    - $\\sigma^2 (Normal) = 2/fan_{in}$\n",
    "- **LeCun**\n",
    "    - Activation functions: SELU\n",
    "    - $\\sigma^2 (Normal) = 1/fan_{in}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonsaturating Activating Functions\n",
    "- Although behavoring well in most deep learning models, the ReLU activation function is not perfect. It suffers from a problem known as the **dying ReLUs**. \n",
    "    - During training, some neurons effectively die, meaning they stop outputting anything other than 0. \n",
    "    - A neuron dies when its weights get tweaked in such a way that **the weighted sum of its inputs are negative for all instances** in the training set. \n",
    "    - When this happens, it just **keeps outputting 0s**, and gradient descent does not affect it anymore since the gradient of the ReLU function is 0 when its input is negative.\n",
    "    \n",
    "## Leaky ReLU\n",
    "- To solve this problem, you may want to use a variant of the ReLU function, such as the **leaky ReLU**. \n",
    "    - This function is defined as $LeakyReLU_\\alpha(z) = max(\\alpha z, z)$. \n",
    "        - The hyperparameter $\\alpha$ defines how much the function “leaks”: it is the slope of the function for $z < 0$, and is typically set to 0.01. \n",
    "        - This small slope ensures that **leaky ReLUs never die**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, 5, -0.5, 4.2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's train a neural network on Fashion MNIST using the **Leaky ReLU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "\n",
    "model.add(keras.layers.Dense(300, kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# tf.random.set_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=[28, 28]),\n",
    "#     keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "#     keras.layers.LeakyReLU(),\n",
    "#     keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "#     keras.layers.LeakyReLU(),\n",
    "#     keras.layers.Dense(10, activation=\"softmax\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "#               optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "#               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Randomized ReLU (RReLU)**\n",
    "    - $\\alpha$ is picked randomly\n",
    "- **Parametric Leaky ReLU (PReLU)**\n",
    "    - $\\alpha$ is authorized to be learned during training.\n",
    "    - Instead of being a hyperparameter, $\\alpha$ becomes a parameter that can be modified by backpropagation like any other parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "model.add(keras.layers.Dense(300, kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.PReLU())\n",
    "\n",
    "model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.PReLU())\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Linear Unit (ELU)\n",
    "- $ELU_\\alpha (z) = \\begin{cases} \\alpha (exp(z) - 1) & z < 0 \\\\ z & z\\geqslant 0 \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha*(np.exp(z)-1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, 5, -2.2, 3.2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We implement ELU in TensorFlow by specifying the activation function when building each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fce93e48b90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='elu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELU\n",
    "- A scaled version of the ELU activation function: $SELU_{scale, alpha}(z) = scale \\times ELU_{alpha}(z)$\n",
    "- This activation function was proposed in this great paper by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. \n",
    "- During training, a neural network composed **exclusively** of a stack of **dense layers** using the **SELU activation function** and **LeCun initialization** will **self-normalize**. \n",
    "    - The output of each layer will tend to preserve the **same mean and variance** during training, which solves the vanishing/exploding gradients problem. \n",
    "- As a result, **SELU function outperforms** the other activation functions very significantly for such neural nets. \n",
    "- Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use $l^1$ or $l^2$ regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). \n",
    "    - However, in practice it works quite well with **sequential ANNs**. \n",
    "    - If you break self-normalization, SELU will not necessarily outperform other activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, 5, -2.2, 3.2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, the SELU hyperparameters (scale and alpha) are tuned in such a way that the **mean output of each neuron** remains **close to 0**, and the **standard deviation** remains **close to 1** (assuming the **inputs are standardized with mean 0 and standard deviation 1** too). \n",
    "- Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, standard deviation 1.00\n",
      "Layer 100: mean 0.02, standard deviation 0.96\n",
      "Layer 200: mean 0.01, standard deviation 0.90\n",
      "Layer 300: mean -0.02, standard deviation 0.92\n",
      "Layer 400: mean 0.05, standard deviation 0.89\n",
      "Layer 500: mean 0.01, standard deviation 0.93\n",
      "Layer 600: mean 0.02, standard deviation 0.92\n",
      "Layer 700: mean -0.02, standard deviation 0.90\n",
      "Layer 800: mean 0.05, standard deviation 0.83\n",
      "Layer 900: mean 0.02, standard deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # assuming standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100)) #LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "#     print(Z.shape)\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, standard deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To implement SELU activation, just set `activation=\"selu\"` and `kernel_initializer=\"lecun_normal\"` when creating a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fce94801b50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation='selu', kernel_initializer='lecun_normal'))\n",
    "\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "    \n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To train a model using the SELU activation function, we need to first **scale the inputs** to mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_std = X_train.std(axis=0, keepdims=True)\n",
    "\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_std\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_std\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train_scaled, y_train, validation_data=(X_valid_scaled, y_valid), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's look what happens if we try to use the ReLU activation function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal'))\n",
    "\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal'))\n",
    "    \n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train_scaled, y_train, validation_data=(X_valid_scaled, y_valid), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing the results above, we can see that SELU outperforms ReLU in terms of avoiding Vanishing/Exploding Gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "- Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.\n",
    "- **Batch Normalization** is another technique to address the vanishing/exploding gradient problem.\n",
    "    - The technique simply **zero-centers and normalizes each input**, then **scaling and shifting the result** using two new parameter vectors per layer: one for scaling, the other for shifting.\n",
    "    - In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. \n",
    "    - It does so by evaluating the mean and standard deviation of each input over the current mini-batch.\n",
    "- Batch Normalization algorithm\n",
    "    1. $\\mu_B = \\frac{1}{m_B}\\sum_{i=1}^{m_B}x^{(i)}$\n",
    "    2. $\\sigma_B^2 = \\frac{1}{m_B}\\sum_{i=1}^{m_B}(x^{(i)}-\\mu_B)^2$\n",
    "    3. $\\hat{(x)}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
    "    4. $z^{(i)} = \\gamma \\times \\hat{(x)}^{(i)} + \\beta$\n",
    "        - $\\mu_B$ is the vector of input means, evaluated over the whole mini-batch B.\n",
    "            - It contains one mean per input (feature).\n",
    "        - $\\sigma_B$ is the vector of input standard deviations, also evaluated over the whole mini-batch.\n",
    "            - It contains one standard deviation per input.\n",
    "        - $m_B$ is the number of instances in the mini_batch.\n",
    "        - $\\hat{(x)}^{(i)}$ is the vector of zero-centered and normalized inputs for instance i.\n",
    "        - $\\gamma$ is the output scale parameter vector for the layer.\n",
    "            - It contains one scale parameter per input.\n",
    "        - $\\times$ represents element-wise multiplication.\n",
    "            - Each input is multiplied by its corresponding output scale parameter.\n",
    "        - $\\beta$ is the output shift (offset) parameter vector for the layer.\n",
    "            - It contains one offset parameter per input. \n",
    "            - Each input is offset by its corresponding shift parameter.\n",
    "        - $\\epsilon$ is a tiny number to avoid division by zero.\n",
    "            - AKA the smoothing term.\n",
    "        - $z^{(i)}$ is the output of the BN operation.\n",
    "            - It's a scaled and shifted version of the inputs of instance i.\n",
    "- To sum up, four parameter vectors are learned in each batch-normalized layer: \n",
    "    - $\\gamma$ (the **ouput scale vector**) and $\\beta$ (the **output offset vector**) are learned through regular **backpropagation**.\n",
    "    - $\\mu$ (the **final input mean vector**) and $\\sigma$ (the **final input standard deviation vector**) are estimated using an **exponential moving average**.\n",
    "\n",
    "## Implementing Batch Normalization with Keras\n",
    "- To implement Batch Normalization using Keras, just add a `BatchNormalization` layer before or after each hidden layer’s activation function, and optionally add a BN layer as well as the first layer in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you display the model summary, you can see that each BN layer adds 4 parameters per input: $\\gamma$, $\\beta$, $\\mu$, and $\\sigma$.\n",
    "    - E.g. The first BN layer adds 3136 parameters, which is 4 times 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let’s look at the parameters of the first BN layer. \n",
    "- Two are trainable (by backprop), and two are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes **applying BN before the activation function** works better (there's a debate on this topic).\n",
    "- Moreover, since a Batch Normalization layer includes one offset parameter per input, you can **remove the bias term** from the previous layer (just pass `use_bias=False` when creating it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal', use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal', use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 20s 363us/sample - loss: 0.4763 - accuracy: 0.8281 - val_loss: 0.3367 - val_accuracy: 0.8746\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 20s 358us/sample - loss: 0.3656 - accuracy: 0.8667 - val_loss: 0.3386 - val_accuracy: 0.8736\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 20s 355us/sample - loss: 0.3282 - accuracy: 0.8787 - val_loss: 0.3127 - val_accuracy: 0.8848\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 20s 359us/sample - loss: 0.3030 - accuracy: 0.8869 - val_loss: 0.3047 - val_accuracy: 0.8880\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 20s 362us/sample - loss: 0.2817 - accuracy: 0.8943 - val_loss: 0.3015 - val_accuracy: 0.8892\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 20s 359us/sample - loss: 0.2633 - accuracy: 0.9022 - val_loss: 0.2996 - val_accuracy: 0.8900\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 20s 358us/sample - loss: 0.2504 - accuracy: 0.9049 - val_loss: 0.2994 - val_accuracy: 0.8916\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 20s 356us/sample - loss: 0.2379 - accuracy: 0.9120 - val_loss: 0.2933 - val_accuracy: 0.8936\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 20s 362us/sample - loss: 0.2262 - accuracy: 0.9155 - val_loss: 0.3030 - val_accuracy: 0.8922\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 20s 365us/sample - loss: 0.2177 - accuracy: 0.9185 - val_loss: 0.3014 - val_accuracy: 0.8926\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fce5210a150>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.DataFrame(history.history)\n",
    "losses[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Clipping\n",
    "- **Gridient Clipping** is another technique to lessen the **exploding gradient** problem by **clipping the gradient duing backpropagation.**\n",
    "- In Keras, implementing Gradient Clipping is just a matter of setting the `clipvalue` or `clipnorm` argument when creating an optimizer.\n",
    "    - Note: If you want to ensure that Gradient Clipping **preserves the direction of the gradient vector**, you should clip by norm by setting **`clipnorm`** instead of clipvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Pretrained Layers\n",
    "- When training neural networks, we should always try to find an **existing neural network** that accomplishes a similar task to the one you are trying to tackle, then just **reuse the lower layers** of this network: this is called **transfer learning**. \n",
    "    - It will not only speed up training considerably, but will also require much less training data.\n",
    "- The right number of layers to reuse\n",
    "    - Try **freezing all** the reused layers first (i.e., make their weights non-trainable, so gradient descent won’t modify them), then train your model and see how it performs.\n",
    "    - Then try **unfreezing one or two of the top hidden layers** to let backpropagation tweak them and see if performance improves.\n",
    "        - The more training data available, the more layers you can unfreeze.\n",
    "        - It is also useful to **reduce the learning rate** when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.\n",
    "    - If you still cannot get good performance, and you have little training data, try **dropping the top hidden layer(s)** and **freeze all remaining hidden layers** again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing a Keras Model\n",
    "- Let's split the Fashion MNIST dataset in two:\n",
    "    - `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "    - `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "- The validation set and the test set are also split this way, but without restricting the number of images. \n",
    "- We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). \n",
    "     - We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). \n",
    "     - However, since we are using Dense layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    \n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation='selu'))\n",
    "\n",
    "model_A.add(keras.layers.Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43986 samples, validate on 4014 samples\n",
      "Epoch 1/10\n",
      "43986/43986 [==============================] - 13s 302us/sample - loss: 0.5909 - accuracy: 0.8100 - val_loss: 0.3772 - val_accuracy: 0.8707\n",
      "Epoch 2/10\n",
      "43986/43986 [==============================] - 12s 277us/sample - loss: 0.3521 - accuracy: 0.8793 - val_loss: 0.3394 - val_accuracy: 0.8779\n",
      "Epoch 3/10\n",
      "43986/43986 [==============================] - 12s 273us/sample - loss: 0.3167 - accuracy: 0.8890 - val_loss: 0.3018 - val_accuracy: 0.8959\n",
      "Epoch 4/10\n",
      "43986/43986 [==============================] - 11s 259us/sample - loss: 0.2969 - accuracy: 0.8967 - val_loss: 0.2858 - val_accuracy: 0.9031\n",
      "Epoch 5/10\n",
      "43986/43986 [==============================] - 12s 281us/sample - loss: 0.2824 - accuracy: 0.9030 - val_loss: 0.2808 - val_accuracy: 0.9071\n",
      "Epoch 6/10\n",
      "43986/43986 [==============================] - 12s 278us/sample - loss: 0.2720 - accuracy: 0.9073 - val_loss: 0.2683 - val_accuracy: 0.9103\n",
      "Epoch 7/10\n",
      "43986/43986 [==============================] - 12s 274us/sample - loss: 0.2641 - accuracy: 0.9095 - val_loss: 0.2681 - val_accuracy: 0.9103\n",
      "Epoch 8/10\n",
      "43986/43986 [==============================] - 12s 275us/sample - loss: 0.2570 - accuracy: 0.9115 - val_loss: 0.2738 - val_accuracy: 0.9048\n",
      "Epoch 9/10\n",
      "43986/43986 [==============================] - 11s 260us/sample - loss: 0.2514 - accuracy: 0.9143 - val_loss: 0.2587 - val_accuracy: 0.9118\n",
      "Epoch 10/10\n",
      "43986/43986 [==============================] - 12s 266us/sample - loss: 0.2456 - accuracy: 0.9162 - val_loss: 0.2594 - val_accuracy: 0.9108\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=10, validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a new model that's entirely new and for task B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 2s 9ms/sample - loss: 0.9545 - accuracy: 0.4600 - val_loss: 0.6655 - val_accuracy: 0.5385\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.5899 - accuracy: 0.6900 - val_loss: 0.4785 - val_accuracy: 0.8519\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.4512 - accuracy: 0.8800 - val_loss: 0.4098 - val_accuracy: 0.8945\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.3871 - accuracy: 0.9100 - val_loss: 0.3666 - val_accuracy: 0.9128\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.3438 - accuracy: 0.9250 - val_loss: 0.3315 - val_accuracy: 0.9300\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.3095 - accuracy: 0.9300 - val_loss: 0.3034 - val_accuracy: 0.9402\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.2810 - accuracy: 0.9400 - val_loss: 0.2808 - val_accuracy: 0.9432\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.2580 - accuracy: 0.9500 - val_loss: 0.2618 - val_accuracy: 0.9462\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.2372 - accuracy: 0.9600 - val_loss: 0.2447 - val_accuracy: 0.9513\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.2196 - accuracy: 0.9650 - val_loss: 0.2316 - val_accuracy: 0.9513\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=10,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, let's create a new model based on the model A’s layers. \n",
    "    - Let’s reuse all layers except for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A = keras.models.Sequential(layers=model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you **train model_B_on_A**, it will also **affect model_A**. \n",
    "- If you want to avoid that, you need to **clone model_A** before you reuse its layers. \n",
    "    - To do this, you must clone model A’s architecture, then copy its weights (since `clone_model()` does not clone the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we could just train model_B_on_A for task B, but since the **new output layer** was **initialized randomly**, it will make large errors, at least during the first few epochs, so there will be large error gradients that may wreck the reused weights. \n",
    "- To avoid this, one approach is to **freeze the reused layers** during the **first few epochs**, giving the new layer some time to learn reasonable weights. \n",
    "    - To do this, simply set every layer’s `trainable` attribute to False and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 2s 8ms/sample - loss: 0.1862 - accuracy: 0.9650 - val_loss: 0.1896 - val_accuracy: 0.9797\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.1594 - accuracy: 0.9800 - val_loss: 0.1661 - val_accuracy: 0.9828\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.1375 - accuracy: 0.9800 - val_loss: 0.1469 - val_accuracy: 0.9838\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.1199 - accuracy: 0.9850 - val_loss: 0.1310 - val_accuracy: 0.9848\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.1052 - accuracy: 0.9850 - val_loss: 0.1182 - val_accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable=False\n",
    "    \n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, validation_data=(X_valid_B, y_valid_B), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we can **unfreeze the reused layers** (which requires **compiling the model again**) and continue training to fine-tune the reused layers for task B. \n",
    "- After unfreezing the reused layers, it is usually a good idea to **reduce the learning rate**, once again to avoid damaging the reused weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 2s 9ms/sample - loss: 0.0462 - accuracy: 0.9900 - val_loss: 0.0357 - val_accuracy: 0.9899\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9675\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 0.0180 - accuracy: 0.9950 - val_loss: 0.0822 - val_accuracy: 0.9878\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 5.2796e-05 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 0.9909\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 0s 1ms/sample - loss: 2.9584e-05 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 0.9970\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable=True\n",
    "    \n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, validation_data=(X_valid_B, y_valid_B), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, let's compare `model_B_on_A` with `model_B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22956316041946412, 0.955]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007541129923303742, 0.997]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, `model_B_on_A` outperforms `model_B` significantly.\n",
    "- However, keep in mind that transfer learning generally does not work very well with small dense networks: it works best with deep convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Pretraining\n",
    "- In the case where you don’t have much labeled training data, and you cannot find a model trained on a similar task, you may still be able to perform **unsupervised pretraining** if you can find enough unlabeled training data.\n",
    "- You can try to **train the layers one by one**, starting with the lowest layer and then going up, using an **unsupervised feature detector algorithm** such as **Restricted Boltzmann Machines (RBMs)** or **autoencoders**. \n",
    "    - Each layer is trained on the output of the previously trained layers (all layers except the one being trained are **frozen**). \n",
    "- Once all layers have been trained this way, you can **add the output layer** for your task, and fine-tune the final network using supervised learning (i.e., with the labeled training examples). \n",
    "    - At this point, you can **unfreeze** all the pretrained layers, or just some of the upper ones.\n",
    "    \n",
    "# Pretraining on an Auxiliary Task\n",
    "- If you do not have much labeled training data, one last option is to train a first neural network on an auxiliary task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers\n",
    "- Training a very large deep neural network can be painfully slow. \n",
    "- So far, we have seen 4 ways to speed up training (and reach a better solution): \n",
    "    - Applying a **good initialization strategy** for the connection weights\n",
    "    - Using a **good activation function**\n",
    "    - Using **Batch Normalization**\n",
    "    - Reusing parts of a **pretrained network** \n",
    "        - Possibly built on an auxiliary task \n",
    "        - Or using unsupervised learning\n",
    "- Another huge speed boost comes from using a **faster optimizer** than the regular Gradient Descent optimizer.\n",
    "\n",
    "## Momentum Optimization\n",
    "- Recall that **regular Gradient Descent** will simply take small regular steps down the slope, as the algorithm simply updates the weights $\\theta$ by directly subtracting the gradient of the cost function $J(\\theta)$ with regards to the weights $(\\nabla_{\\theta} J(\\theta))$ multiplied by the learning rate $\\eta$.\n",
    "    - $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta)$\n",
    "- **Momentum optimization** cares a great deal about what **previous gradients** were. \n",
    "- At each iteration, it subtracts the local gradient from the **momentum vector** $m$ (multiplied by the learning rate $\\eta$), and it updates the weights by simply adding this momentum vector.\n",
    "    - $m \\leftarrow \\beta m - \\eta \\nabla_{\\theta} J(\\theta)$\n",
    "    - $\\theta \\leftarrow \\theta + m$\n",
    "        - $\\beta$ is the new hyperparamter **momentum** with values between 0 and 1 (typical value is 0.9).\n",
    "- Although Momentum optimization adds another hyperparameter to tune, the **momentum value of 0.9** usually works well in practice and almost always goes faster than regular Gradient Descent.\n",
    "- To implement momentum optimization using Keras, just use the **SGD optimizer** and set its `momentum` hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x7fce766c3690>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.SGD(lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient (NAG)\n",
    "- One small variant to Momentum optimization, almost always **faster than vanilla Momentum optimization**. \n",
    "- The idea of **Nesterov Momentum optimization**, or **Nesterov Accelerated Gradient (NAG)**, is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum. \n",
    "    - The only difference from vanilla Momentum optimization is that the gradient is measured at $\\theta + \\beta m$ rather than at $\\theta$.\n",
    "        - $m \\leftarrow \\beta m - \\nabla_{\\theta}J(\\theta + \\beta m) \\times \\eta$\n",
    "        - $\\theta \\leftarrow \\theta + m$\n",
    "- To use it, simply set `nesterov=True` when creating the **SGD** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x7fce766c3d50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.SGD(lr=1e-3, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "- The AdaGrad algorithm **corrects its direction** to point a bit more toward the **global optimum** by **scaling down the gradient vector** along the steepest dimensions.\n",
    "- The first step of the algorithm accumulates the square of the gradient, and the second step scales down the gradient vector.\n",
    "    - $s \\leftarrow s + \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta}J(\\theta)$\n",
    "    - $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}J(\\theta) \\oslash \\sqrt{s + \\epsilon}$\n",
    "- In short, this algorithm **decays the learning rate**, but it does so **faster for steep dimensions** than for dimensions with gentler slopes. \n",
    "    - This is called an **adaptive learning rate**.\n",
    "- Note that AdaGrad tends to stop too early when training deep neural networks.\n",
    "    - So even though Keras has an Adagrad optimizer, you should **NOT** use it to train **deep neural networks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad at 0x7fce766c82d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.Adagrad(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "- Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm fixes this by **accumulating only the gradients** from **the most recent iterations** (as opposed to all the gradients since the beginning of training). \n",
    "    - $s \\leftarrow \\beta s + (1-\\beta) \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta}J(\\theta)$\n",
    "    - $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}J(\\theta) \\oslash \\sqrt{s + \\epsilon}$\n",
    "        - $\\beta$ is the decay rate, typically set 0.9.\n",
    "- The RMSProp optimizer almost always performs much better than AdaGrad.\n",
    "- To implement RMSProp, we can use the `RMSProp` optimizer from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop at 0x7fce766c8910>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.RMSprop(lr=0.01, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam and Nadam Optimization\n",
    "- **Adam**, which stands for **adaptive moment estimation**, combines the ideas of **Momentum optimization** and **RMSProp**.\n",
    "    - Just like Momentum optimization it keeps track of an **exponentially decaying average of past gradients**\n",
    "    - Just like RMSProp it keeps track of an **exponentially decaying average of past squared gradients**.\n",
    "- The Adam algorithm\n",
    "    1. $m \\leftarrow \\beta_1 m - (1-\\beta_1) \\nabla_{\\theta}J(\\theta)$\n",
    "    2. $s \\leftarrow \\beta_2 s + (1-\\beta_2) \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta}J(\\theta)$\n",
    "    3. $\\hat{m} \\leftarrow \\frac{m}{1-{\\beta_1}^t}$\n",
    "    4. $\\hat{s} \\leftarrow \\frac{s}{1-{\\beta_2}^t}$\n",
    "    5. $\\theta \\leftarrow \\theta + \\eta \\hat{m} \\oslash \\sqrt{\\hat{s}+\\epsilon}$\n",
    "        - The momentum decay hyperparameter $\\beta_1$ is typically initialized to 0.9.\n",
    "        - The scaling decay hyperparameter $\\beta_2$ is often initialized to 0.999. \n",
    "        - As earlier, the smoothing term $\\epsilon$ is usually initialized to a tiny number such as $10^{-7}$.\n",
    "- To implement Adam optimization, we can use the `Adam` class from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x7fce766c8e50>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Nadam optimization** is simply **Adam optimization** plus the **Nesterov trick**, so it will often **converge slightly faster** than Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.nadam.Nadam at 0x7fce766cb450>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.Nadam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduling\n",
    "- Finding the optimal learning rate is tricky, and wee can do better than a constant learning rate.\n",
    "    - If you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. \n",
    "- There are many different **strategies to reduce the learning rate** during training, which are called **learning schedules**.\n",
    "\n",
    "## Power Scheduling\n",
    "- $\\eta(t) = \\frac{\\eta_0}{(1+t/k)^c}$\n",
    "- This schedule first drops quickly, then more and more slowly as $t$ increases.\n",
    "- Implementing power scheduling in Keras is the easiest option: just set the `decay` hyperparameter when creating an optimizer. \n",
    "    - The `decay` is the inverse of $k$ (the number of steps it takes to divide the learning rate by one more unit), and Keras assumes that $c$ is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x7fce93eb92d0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.optimizers.SGD(lr=0.01, decay=1/10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Scheduling\n",
    "- $\\eta(t) = \\eta_0 0.1^{t/s}$\n",
    "- The learning rate will gradually drop by a factor of 10 every $s$ steps.\n",
    "- To implement exponential scheduling:\n",
    "    - First, define a function that takes the current epoch and returns the learning rate\n",
    "    - Next, create a **`LearningRateScheduler` callback**, giving it the schedule function, and pass this callback to the `fit()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0*0.1**(epoch/s)\n",
    "    return exponential_decay_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, ..., callbacks=[exp_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The schedule function can take the **current learning rate** as a second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    " def exponential_decay_fn(epoch, lr):\n",
    "        return lr*0.1**(1/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you want to **update the learning rate at each iteration** rather than each epoch, you must write your own **callback class**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, lr*0.1**(1/s))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 20 * len(X_train) // 32   # number of steps in 20 epochs, given batch size is 32 (one batch in each step)\n",
    "\n",
    "exp_decay = ExponentialDecay(s)\n",
    "\n",
    "# history = model.fit(X_train, y_train, ..., callbacks=[exp_decay])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise Constant Scheduling\n",
    "- Use a constant learning rate for a number of epochs (e.g. $\\eta_0=0.1$ for 5 epochs), then a smaller learning rate for another number of epochs (e.g. $\\eta_1=0.001$ for 50 epochs), and so on.\n",
    "- To implement Piecewise Constant Scheduling, we can use a **schedule function** as earlier, then create a **`LearningRateScheduler` callback** with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "piecewise_constant_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, ..., callbacks=[piecewise_constant_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Scheduling\n",
    "- Measure the validation error every $N$ steps, and reduce the learning rate by a factor $\\lambda$ when the error stops dropping.\n",
    "- To implement performance scheduling, simply use the **`ReduceLROnPlateau` callback**. \n",
    "    - The following callback will multiply the learning rate by 0.5 ($\\lambda = 0.5$) whenever the best validation loss does not improve for 5 consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "# history = model.fit(X_train, y_train, ..., callbacks=[performance_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras Schedulers\n",
    "- Lastly, **tf.keras** offers an alternative way to implement learning rate scheduling: just define the learning rate using one of the schedules available in `keras.optimisers.schedules`, then pass this learning rate to any optimizer.\n",
    "- This approach updates the learning rate at **each step** rather than at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "s = n_epochs * len(X_train) // batch_size\n",
    "\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, \n",
    "                                                            decay_steps=s, decay_rate=0.1)\n",
    "exp_optimizer = keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer=exp_optimizer, metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(X_train, y_train, ..., epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting through Regularization\n",
    "## $l_1$ and $l_2$ Regularization\n",
    "- Just like you did in Chapter 4 for simple linear models, you can use $l_1$ and $l_2$ regularization.\n",
    "- We implement $l_1$ and $l_2$ regularizations using `keras.regularizers.l1()` and `keras.regularizers.l2()`.\n",
    "    - We need to specify the **regularization factor**.\n",
    "    - These functions will compute the regularization loss at each step during training, which is then added to the final loss.\n",
    "- If you want **both $l_1$ and $l_2$ regularization**, use `keras.regularizers.l1_l2()`, specifying both regularization factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fce74087390>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", \n",
    "                   kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since you will typically want to apply the **same regularizer, activation function, and initialization strategy** to **all layers** in your network, you may find yourself repeating the same arguments over and over.\n",
    "- To avoid this, you can try refactoring your code to use loops. \n",
    "- Another option is to use Python’s `functools.partial()` function.\n",
    "    - It lets you create a thin wrapper for any callable, with some default argument values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                          activation='elu',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "model.add(RegularizedDense(300))\n",
    "model.add(RegularizedDense(300))\n",
    "model.add(RegularizedDense(300))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "- Dropout is one of the most popular regularization techniques for deep neural networks.\n",
    "- It is a fairly simple algorithm: \n",
    "    - At **every training step**, every neuron (including the **input neurons**, but always excluding the output neurons) has a **probability $p$** of being temporarily “dropped out,” meaning it will be **entirely ignored** during this training step, but it may be active during the next step. \n",
    "    - The hyperparameter $p$ is called the **dropout rate**, and it is typically set to 50%.\n",
    "- There is one small but important technical detail: Suppose p = 50%, in which case during testing a neuron will be connected to twice as many input neurons as it was (on average) during training. \n",
    "    - To compensate for this fact, we need to **multiply each input connection weight** by the **keep probability** $(1 – p)$ after training. \n",
    "    - Alternatively, we can **divide each neuron’s output** by the **keep probability** during training.\n",
    "- To implement dropout using Keras, you can use the `keras.layers.Dropout` layer.\n",
    "    - During training, it randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "model.add(keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo (MC) Dropout\n",
    "- To implement MC Dropout, we first **force training mode on**, using a `learning_phase_scope(1)` context. \n",
    "    - This turns dropout on within the `with` block. \n",
    "- Then we **make 100 predictions** over the test set, and we **stack them**. \n",
    "    - Since dropout is on, all predictions will be different. \n",
    "    - Recall that `predict()` returns a matrix with one row per instance, and one column per class.\n",
    "    - Since there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape [10000, 10]. \n",
    "    - We stack 100 such matrices, so y_probas is an array of shape [100, 10000, 10]. \n",
    "- Once we average over the first dimension (`axis=0`), we get `y_proba`, an array of shape [10000, 10], like we would get with a single prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with keras.backend.learning_phase_scope(1):\n",
    "#     y_probas = np.stack([model.predict(X_test_scaled) for sample in range(100)])\n",
    "\n",
    "# y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If your model contains **other layers that behave in a special way** during training (such as **Batch Normalization layers**), then you should NOT force training mode like we just did. \n",
    "- Instead, you should replace the Dropout layers with the following **MCDropout class**.\n",
    "    - We just sublass the Dropout layer and override the `call()` method to force its `training` argument to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you are creating a model from scratch, it’s just a matter of using `MCDropout` rather than `Dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "model.add(keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(MCDropout(0.5))\n",
    "model.add(keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(MCDropout(0.5))\n",
    "model.add(keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(MCDropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax', kernel_initializer='glorot_uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_10 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "mc_dropout (MCDropout)       (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "mc_dropout_1 (MCDropout)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "mc_dropout_2 (MCDropout)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 419,110\n",
      "Trainable params: 419,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you have a model that was already trained using `Dropout`, you need to create a new model identical to the existing model except replacing the `Dropout` layers with `MCDropout`, then copy the existing model’s weights to your new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCDropout(layer.rate) if isinstance(layer, keras.layers.Dropout) else layer for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_10 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "mc_dropout_3 (MCDropout)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "mc_dropout_4 (MCDropout)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "mc_dropout_5 (MCDropout)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 419,110\n",
      "Trainable params: 419,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "55000/55000 [==============================] - 25s 462us/sample - loss: 1.0600 - accuracy: 0.7161 - val_loss: 0.6135 - val_accuracy: 0.7926\n",
      "Epoch 2/3\n",
      "55000/55000 [==============================] - 24s 442us/sample - loss: 0.5843 - accuracy: 0.7939 - val_loss: 0.5780 - val_accuracy: 0.7980\n",
      "Epoch 3/3\n",
      "55000/55000 [==============================] - 24s 440us/sample - loss: 0.5655 - accuracy: 0.8054 - val_loss: 0.5299 - val_accuracy: 0.8216\n"
     ]
    }
   ],
   "source": [
    "history = mc_model.fit(X_train_scaled, y_train, validation_data=(X_valid_scaled, y_valid), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.95]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.12, 0.  , 0.78]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range (100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Norm Regularization\n",
    "- For **each neuron**, it **constrains the weights** $w$ of the incoming connections such that $\\left\\|w \\right\\|_2 \\leq r$, where $r$ is the max-norm hyperparameter and $\\left\\|w \\right\\|_2$ is the $l_2$ norm.\n",
    "- Max-norm regularization does not add a regularization loss term to the overall loss function. \n",
    "    - Instead, it is typically implemented by computing $\\left\\|w \\right\\|_2$ after each training step and clipping $w$ if needed ($w \\leftarrow w\\frac{r}{\\left\\|w \\right\\|_2}$).\n",
    "    - Reducing $r$ increases the amount of regularization and helps reduce overfitting.\n",
    "- To implement max-norm regularization in Keras, just set every hidden layer’s `kernel_constraint` argument to a `max_norm()` constraint, with the appropriate max value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal',\n",
    "                          kernel_constraint=keras.constraints.max_norm(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, let's build an ANN model using Max-Norm regularization using the `functools.partial()` function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                      activation='selu',\n",
    "                      kernel_initializer='lecun_normal',\n",
    "                      kernel_constraint=keras.constraints.max_norm(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\n",
    "model.add(MaxNormDense(300))\n",
    "model.add(MaxNormDense(100))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "55000/55000 [==============================] - 22s 402us/sample - loss: 0.4761 - accuracy: 0.8338 - val_loss: 0.3973 - val_accuracy: 0.8544\n",
      "Epoch 2/3\n",
      "55000/55000 [==============================] - 21s 379us/sample - loss: 0.3584 - accuracy: 0.8691 - val_loss: 0.3782 - val_accuracy: 0.8672\n",
      "Epoch 3/3\n",
      "55000/55000 [==============================] - 21s 382us/sample - loss: 0.3280 - accuracy: 0.8785 - val_loss: 0.3497 - val_accuracy: 0.8752\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=3, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37800401457548144, 0.8634]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Practical Guidelines\n",
    "- The configuration below will work fine in most cases, without requiring much hyperparameter tuning.\n",
    "    - Kernel initializer: **LeCun Initialization**\n",
    "    - Activation function: **SELU**\n",
    "    - Normalization: None (self-normalization)\n",
    "    - Regularization: **Early Stopping**\n",
    "    - Optimizer: **Nadam**\n",
    "    - Learning rate schedule: **Performance Scheduling**\n",
    "    - Also, don't forget to **standardize the input features**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "## 8. Deep Learning\n",
    "- This exercise requires fitting models for hundreds of epochs, which is highly computationaly-expensive.\n",
    "- For the sample code, visit: https://github.com/ageron/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
